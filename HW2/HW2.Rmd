---
title: "Homework 2"
author: "Hyungkyu Lim"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE ,tidy.opts=list(width.cutoff = 60), tidy = TRUE)
library(leaps)
library(glmnet)
```
## Question 1
(10 points) (Exercise 9 modified, ISL) In this exercise, we will predict the numberof applicatiions received using the other variables in the College data set in the ISLR package.
(a) Split the data set into a training set and a test set. Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
library(ISLR)
attach(College)
set.seed(12345)

split_data <- sample(nrow(College),nrow(College)*0.7)
train_model <- College[split_data,]
test_model <- College[-split_data,]


ls_model <- lm(Apps~., data =train_model)
summary(ls_model)

predicted <- predict(ls_model,test_model)

error <- mean((test_model$Apps-predicted)^2)

error
```
(b) Fit a ridge regression model on the training set, with lambda chosen by crossvalidation. Report the test error obtained.
```{r}
library(glmnet)

train_ridge <- model.matrix(Apps~., data=train_model)
test_ridge<- model.matrix(Apps~., data=test_model)


ridge.mod <-glmnet(train_ridge, train_model$Apps, alpha=0)

cv.ridge <- cv.glmnet(train_ridge,train_model$Apps,alpha=0)

plot(cv.ridge)

ridge_bestlam <- cv.ridge$lambda.min

ridge_bestlam

pred_ridge <- predict(cv.ridge, s= ridge_bestlam, newx=test_ridge) 

ridge_test_error <- mean((test_model$Apps-pred_ridge)^2)

ridge_test_error




```

(d) Fit a lasso model on the training set, with lambda chosen by crossvalidation.
Report the test error obtained, along with the number of non-zero coefficient
estimates.
```{r}
train_lasso <- model.matrix(Apps~., data=train_model)

test_lasso<- model.matrix(Apps~., data=test_model)

lasso_model <- glmnet(train_lasso, train_model$Apps, alpha=1)

cv.lasso <- cv.glmnet(train_lasso, train_model$Apps, alpha=1)

plot(cv.lasso)

lasso_bestlam <- cv.lasso$lambda.min

lasso_bestlam

pred_lasso<-predict(lasso_model,s=lasso_bestlam,newx =test_lasso)

lasso_test_error <- mean((test_model$Apps-pred_lasso)^2)

lasso_test_error

predict(lasso_model,s=lasso_bestlam,type="coefficients")
```

(e) Fit a PCR model on the training set, with k chosen by cross-validation. Report the test error obtained, along with the value of k selected by cross-validation.
```{r}
library(pls)
set.seed(2)

pcr_model = pcr(Apps~. , data = College, scale = TRUE, validation = "CV")
summary(pcr_model)
validationplot(pcr_model, val.type = "MSEP")

train <- sample(nrow(College),nrow(College)*0.7)
test=-train
y_test = College$Apps[test]
y_train = College$Apps[train]


pcr_model <- pcr(Apps~., data=College, subset=train ,sclae=TRUE, validation="CV")
summary(pcr_model)

training_error_store <- c()
test_error_store <- c()
for (i in 1:17){
	pcr_pred_test = predict(pcr_model, College[test,], ncomp = i)
	test_error <- mean((pcr_pred_test-y_test)^2)
	test_error_store <- c(test_error_store, test_error)
}


plot(test_error_store)

pcr_pred=predict(pcr_model,College[test,],cnomp=5)

pcr_test_error <- mean((pcr_pred-y_test)^2)


```
(f) Fit a PLS model on the training set, with k chosen by crossvalidation.
Report the test error obtained, along with the value of k selected by cross-validation
```{r}
pls_model = plsr(Apps ~., data = College, subset = train, scale = TRUE, validation = "CV")
summary(pls_model)
validationplot(pls_model, val.type = "MSEP")

training_error_store_pls <- c()
test_error_store_pls <- c()
for (i in 1:17){
	pls_pred_test = predict(pls_model, College[test,], ncomp = i)
	test_error_pls <- mean((pls_pred_test-y_test)^2)
	test_error_store_pls <- c(test_error_store_pls, test_error_pls)
}


plot(test_error_store_pls)

pls_pred=predict(pls_model,College[test,],cnomp=8)

pls_test_error <- mean((pls_pred-y_test)^2)


error_combined <- c(error,ridge_test_error, lasso_test_error, pcr_test_error, pls_test_error)


plot(error_combined, main ="MSE" ,xlab = "1: linear, 2: ridge, 3: lasso, 4: pcr, 5: pls",col="red")



```
##Question 2
(10 points) The insurance company benchmark data set gives information on
customers. Specifically, it contains 86 variables on product-usage data and sociodemographic data derived from zip area codes. There are 5,822 customers in the training set and another 4,000 in the test set. The data were collected to answer the following questions: Can you predict who will be interested in buying a
caravan insurance policy and give an explanation why? Compute the OLS
estimates and compare them with those obtained from the following variableselection algorithms: Forwards Selection, Backwards Selection, Lasso regression, and Ridge regression. Support your answer.

```{r}
train_data <- read.table("~/desktop/EAS506/ticdata2000.txt")
train_y <- train_data$V86
train_x <- train_data[-86]


test_x <- read.table("~/desktop/EAS506/ticeval2000.txt")
test_y <- read.table("~/desktop/EAS506/tictgts2000.txt")

linear_mod <- lm(V86~., data=train_data)
summary(linear_mod)

train_y_pred <- predict(linear_mod, newdata=train_x)


hist(train_y_pred)

which(train_y_pred>0.5)



summary(train_y_pred)

train_error_lm <- mean((train_y_pred-train_y)^2)

train_error_lm

test_y_pred <- predict(linear_mod,newdata=test_x)

summary(test_y_pred)

hist(test_y_pred)

max(test_y_pred)

which(test_y_pred>0.5)

test_error_lm <- mean((test_y_pred-test_y)^2)

test_error_lm
```

```{r}
##Forward 


regfit_forward <- regsubsets(V86~., data=train_data,nvmax=86, method="forward")

reg_summary =summary(regfit_forward)

par(mfrow=c(1,2))

plot(reg_summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")

plot(reg_summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")


which(reg_summary$cp == min(reg_summary$cp))


which(reg_summary$bic == min(reg_summary$bic)) 


coef_forward_bic<- coef(regfit_forward, 8)

coef_forward_bic

forward_lm <- lm(V86~ V10 + V18 + V43 + V44 + V47 + V59 + V82 + V85, data=train_data)


train_y_pred_forward<- predict(forward_lm,newdata = train_x)

forward_train_error <- mean((train_y_pred_forward=train_y)^2)

forward_train_error

test_y_pred_forward <- predict(forward_lm, newdata = test_x)

forward_test_error <- mean((test_y_pred_forward-test_y)^2)

forward_test_error
```

```{r}
##Backward

regfit_backward <- regsubsets(V86~., data=train_data,nvmax=86, method="backward")

reg_summary_back =summary(regfit_backward)

par(mfrow=c(1,2))

plot(reg_summary_back$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")

plot(reg_summary_back$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")


which(reg_summary_back$cp == min(reg_summary_back$cp))


which(reg_summary_back$bic == min(reg_summary_back$bic)) 


coef_backward_bic<- coef(regfit_backward, 8)
coef_backward_bic

backward_lm <- lm(V86~ V10 + V18 + V21 + V46 + V47 + V59 + V82 + V85, data=train_data)

train_y_pred_backward<- predict(backward_lm,newdata = train_x)

backward_train_error <- mean((train_y_pred_backward-train_y)^2)

backward_train_error

test_y_pred_backward <- predict(backward_lm, newdata = test_x)

backward_test_error <- mean((test_y_pred_backward-test_y)^2)

backward_test_error
```

```{r}
#Ridge

train_ridge_x <- as.matrix(train_data[,c(1:85)])
train_ridge_y <- train_data[,c(86)]

test_ridge_x <- as.matrix(test_x)
test_ridge_y <- test_y 

cv.ridge <- cv.glmnet(train_ridge_x, train_ridge_y, alpha = 0)
plot(cv.ridge)



ridge_bestlam <- cv.ridge$lambda.min

ridge_bestlam

ridge.mod = glmnet(train_ridge_x, train_ridge_y, alpha=0)

ridge.pred <- predict(ridge.mod, s= ridge_bestlam, type = "coefficients")

ridge.pred2 <- predict(ridge.mod, s = ridge_bestlam, newx = train_ridge_x, type = "response")


ridge_train_error <- mean((ridge.pred2-train_ridge_y)^2)

ridge_train_error

ridge.pred3 <- predict(ridge.mod, s =ridge_bestlam, newx = test_ridge_x , type = "response")

ridge_test_error <- mean((ridge.pred3 - test_ridge_y)^2) 

ridge_test_error

```

```{r}
#lasso

train_lasso_x <- as.matrix(train_data[,c(1:85)])
train_lasso_y <- train_data[,c(86)]

test_lasso_x <- as.matrix(test_x)
test_lasso_y <- test_y 

cv.lasso = cv.glmnet(train_lasso_x, train_lasso_y, alpha = 1)
plot(cv.lasso)

lasso_bestlam <- cv.lasso$lambda.min
lasso_bestlam

lasso.mod = glmnet(train_lasso_x,train_lasso_y, alpha=1)

lasso.pred <- predict(lasso.mod, s= lasso_bestlam, type = "coefficients")

lasso.pred2 <- predict(lasso.mod, s = lasso_bestlam, newx = train_lasso_x, type = "response")

lasso_train_error <- mean((lasso.pred2-train_lasso_y)^2)

lasso_train_error

lasso.pred3 <- predict(lasso.mod, s =lasso_bestlam, newx = test_lasso_x , type = "response")

lasso_test_error <- mean((lasso.pred3 - test_lasso_y)^2) 

lasso_test_error

```
##Question3
(10 points) (Exercise 9 modified, ISL) We have seen that as the number of
features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.
Generate a data set with p = 20 features, n = 1, 000 observations, and an
associated quantitative response vector generated according to the model Y=XB+ewhere B has some elements that are exactly equal to zero. Split your data set into a training set containing 100 observations and a test set containing 900
observations.Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size. Plot the test set MSE associated with the best model of each size.

```{r}
set.seed(12345)

x <- matrix(rnorm(1000*20),nrow=1000,ncol=20)

b<-rnorm(20)

b[c(2,4,5,7,10,15)] <- 0

random_error <- rnorm(1000)

y<- x%*%b + random_error

train <- sample(seq(1000),100, replace =FALSE)

test <- (-train)

x_train <- x[train,]

x_test <- x[test,]

y_train <- y[train]

y_test <- y[test]

data_train <- data.frame( y = y_train, x = x_train)

reg_fit <- regsubsets(y~., data= data_train, nvmax=20)

train_mat <- model.matrix(y ~., data=data_train,nvmax=20 )

validation_error <- rep(NA,20)
for (i in 1:20) {
  coefi = coef(reg_fit, id=i)
  pred <- train_mat[ , names(coefi)] %*% coefi
  validation_error[i] <- mean((pred-y_train)^2)
} 
#I applied this function from ISR lab chaper6. 

data_test <- data.frame(y = y_test, x = x_test)

test_mat <- model.matrix(y ~., data=data_test,nvmax=20 )

validation_error_test <- rep(NA,20)
for (i in 1:20) {
  coefi = coef(reg_fit, id=i)
  pred <- test_mat[ , names(coefi)] %*% coefi
  validation_error_test[i] <- mean((pred-y_test)^2)
} 
#I applied this function from ISR lab chaper6. 

par(mfrow=c(1,2))

plot(validation_error, xlab = "Number of predictors", ylab = "Training MSE",col="blue",type="l")
plot(validation_error_test, xlab = "Number of predictors", ylab = "Test MSE",col="red",type="l")




```














